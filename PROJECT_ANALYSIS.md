# 프로젝트 상세 분석: 실시간 소셜 미디어 분석 파이프라인

## 1. 프로젝트 개요

이 문서는 '실시간 소셜 미디어 감성 분석 파이프라인' 프로젝트의 목표, 아키텍처, 데이터 흐름, 기술적 결정 및 코드 구조를 비전문가도 이해할 수 있도록 상세히 설명합니다.

### 1.1. 목표

- **실시간 데이터 처리:** 전 세계 사용자들이 생성하는 소셜 미디어(Reddit) 데이터를 지연 시간 없이 수집하고 처리하는 능력을 보여줍니다.
- **대용량 데이터 처리:** 수십, 수백만 건의 데이터를 안정적으로 처리할 수 있는 Spark와 Kafka 기반의 확장 가능한 아키텍처를 구축합니다.
- **다국어 자연어 처리(NLP):** 최신 AI 모델(Hugging Face Transformers)을 활용하여 영어와 한국어 텍스트의 감성을 자동으로 분석하고 분류합니다.
- **엔드투엔드 파이프라인 구축:** 데이터 수집부터 처리, 저장, 모니터링, 시각화에 이르는 데이터 엔지니어링의 전 과정을 직접 설계하고 구현합니다.
- **안정적인 운영:** Airflow를 이용한 워크플로우 자동화 및 모니터링을 통해 파이프라인이 중단 없이 안정적으로 운영될 수 있도록 보장합니다.

### 1.2. 왜 이 프로젝트가 중요한가?

기업은 소셜 미디어를 통해 자사 제품이나 브랜드에 대한 고객의 반응을 실시간으로 파악하고 싶어 합니다. 이 프로젝트는 바로 그 요구사항을 해결하는 기술적인 청사진을 제시합니다. 긍정적인 반응은 마케팅에 활용하고, 부정적인 반응은 신속하게 대응하여 위기를 관리하는 등 실제 비즈니스에 직접적인 가치를 제공할 수 있는 시스템입니다.

---

## 2. 아키텍처 및 구성 요소 상세 설명

이 파이프라인은 여러 전문적인 도구들이 각자의 역할을 수행하며 유기적으로 연결된 구조입니다. 각 구성 요소를 쉽게 비유하여 설명합니다.

| 구성 요소 | 비유 | 역할 및 설명 |
| :--- | :--- | :--- |
| **Docker** | 조립식 주택 단지 | 모든 서비스(Kafka, Spark 등)를 독립된 공간(컨테이너)에서 실행시켜, 어디서든 동일한 환경을 보장하는 기술입니다. |
| **Reddit API** | 뉴스 속보 공급처 | 실시간으로 Reddit에 올라오는 게시글과 댓글 데이터를 제공하는 창구입니다. |
| **Kafka** | 대형 컨베이어 벨트 | Reddit에서 들어오는 데이터를 끊임없이 받아서 다음 단계인 Spark로 안전하고 빠르게 전달하는 역할을 합니다. |
| **Spark Streaming** | 만능 요리사 (실시간) | 컨베이어 벨트(Kafka)에서 내려오는 재료(데이터)를 실시간으로 요리(분석)합니다. 언어를 확인하고, 그에 맞는 레시피(감성분석 모델)를 적용합니다. |
| **HDFS** | 초대형 냉장 창고 | 요리(분석)가 끝난 결과물(데이터)을 `Parquet`이라는 규격화된 박스에 담아 대량으로, 안전하게 보관하는 분산 파일 시스템입니다. |
| **Airflow** | 총괄 매니저 | 파이프라인 전체가 잘 돌아가는지 관리하고 감독합니다. 매일 자정이 되면 어제의 판매 실적(감성 데이터)을 집계하여 보고서를 만들고, 매시간 창고(HDFS)에 물건이 잘 쌓이고 있는지 확인하며, 모든 시스템(Kafka, Spark 등)이 건강한지 체크합니다. |
| **Streamlit** | 레스토랑의 메뉴판 | 창고(HDFS)에 저장된 분석 결과를 손님(사용자)이 보기 좋게 실시간 차트와 그래프로 보여주는 웹 대시보드입니다. |
| **Hugging Face** | 미슐랭 스타 셰프 군단 | 한국어, 영어 등 각 언어에 특화된 최고의 감성 분석 AI 모델(요리법)을 제공합니다. |

---

## 3. 데이터 흐름 (End-to-End)

데이터가 어떤 여정을 거치는지 단계별로 따라가 봅니다.

1.  **수집 (`reddit_producer.py`):**
    - `praw` 라이브러리를 사용하여 Reddit API에 연결합니다.
    - `settings.SUBREDDIT_NAME`에 지정된 서브레딧의 게시글과 댓글을 실시간으로 스트리밍합니다.
    - 수집된 데이터(작성자, 내용, 시간 등)를 JSON 형식으로 만들어 Kafka의 `reddit-stream` 토픽(채널)에 전송합니다.

2.  **처리 (`sentiment_analyzer.py`):**
    - Spark Streaming 작업이 Kafka의 `reddit-stream` 토픽을 구독하고 있다가, 새 데이터가 들어오면 즉시 처리합니다.
    - **1단계 (언어 감지):** `fastText` 모델을 사용하여 데이터의 언어가 한국어(`ko`)인지 영어(`en`)인지, 혹은 기타 언어인지 판별합니다.
    - **2단계 (감성 분석):**
        - 언어가 `ko`이면, `KoELECTRA` 모델을 적용하여 감성을 분석합니다.
        - 언어가 `en`이면, `DistilBERT` 모델을 적용하여 감성을 분석합니다.
        - 그 외 언어는 `unknown`으로 처리합니다.
    - 분석이 완료된 데이터(원본 내용 + 언어 + 감성 레이블)를 HDFS의 `social_sentiment_data` 경로에 `Parquet` 파일 형식으로 저장합니다.

3.  **저장 (HDFS):**
    - 데이터는 HDFS에 시간 순서대로 계속 누적됩니다. Parquet 형식은 압축률이 높고, Spark에서 읽기/쓰기 성능이 매우 뛰어나 대용량 데이터 분석에 적합합니다.

4.  **배치 처리 및 모니터링 (Airflow DAGs):**
    - **`daily_sentiment_report_dag.py`:** 매일 자정, Spark 배치 작업을 실행하여 HDFS에 쌓인 지난 24시간의 데이터를 종합 분석하고, 평균 감성 점수와 인기 키워드를 추출하여 별도의 리포트(`daily_sentiment_reports`)로 저장합니다.
    - **`hourly_data_quality_dag.py`:** 매시간, HDFS에 최근 1시간 동안 데이터가 정상적으로 쌓였는지 확인하여 데이터 누락 여부를 검증합니다.
    - **`daily_service_monitor_dag.py`:** 매일, Kafka, Spark, HDFS 등 핵심 서비스들이 정상적으로 응답하는지 상태를 점검합니다.

5.  **시각화 (`dashboard.py`):**
    - Streamlit 대시보드는 주기적으로 HDFS의 `social_sentiment_data` 경로에서 최신 데이터를 읽어옵니다.
    - 전체 감성 분포, 시간대별 데이터량, 최신 데이터 내용 등을 차트와 표로 시각화하여 사용자에게 보여줍니다.

---

## 4. 주요 코드 및 로직 설명

- **`config/settings.py`:** 프로젝트의 모든 설정이 이곳에 있습니다. Kafka 주소, HDFS 경로, 모델 이름 등을 변경하고 싶다면 이 파일만 수정하면 됩니다. 이는 코드의 유지보수성을 크게 향상시킵니다.
- **`scripts/sentiment_analyzer.py`의 UDF:**
    - `@pandas_udf`를 사용하여 Python 코드를 Spark에서 효율적으로 실행할 수 있도록 만들었습니다.
    - `globals()`를 사용하여 각 Spark Executor(작업자)가 무거운 AI 모델을 단 한 번만 로드하도록 최적화했습니다. 이 기법이 없다면 데이터 레코드마다 모델을 로드하여 엄청난 성능 저하가 발생할 것입니다.
- **`docker-compose.yml`:**
    - `depends_on`과 `healthcheck`를 사용하여 서비스 간의 실행 순서와 의존성을 명확히 정의했습니다. 예를 들어, Spark 분석 서비스는 Kafka와 HDFS가 완전히 준비된 이후에 시작됩니다.
- **`tests/test_dags.py`:**
    - `DagBag`을 이용해 `dags` 폴더의 모든 파일을 실제로 실행하지 않고 로드만 해봄으로써, DAG 정의에 문법적 오류가 없는지 자동으로 검증합니다. 이는 파이프라인의 안정성을 보장하는 첫 번째 방어선입니다.

## 5. 이번 개선 작업의 요약

- **일관성 및 유지보수성 향상:** 하드코딩된 값들을 `settings.py`로 통합하여, 설정 변경이 필요할 때 여러 파일을 헤맬 필요 없이 한 곳에서 관리할 수 있게 되었습니다.
- **안정성 확보:** 테스트 코드를 도입하여, 코드 변경 시 발생할 수 있는 예기치 못한 오류를 사전에 발견하고 수정할 수 있는 체계를 마련했습니다.
- **프로젝트 명확성 증대:** 불필요한 파일을 제거하고, 이 분석 문서와 같이 프로젝트의 구조와 흐름을 명확히 설명하는 문서를 추가하여 누구나 쉽게 프로젝트를 이해하고 기여할 수 있도록 만들었습니다.
