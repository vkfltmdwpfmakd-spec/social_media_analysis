import streamlit as st
import pandas as pd
import requests
import json
import time
import os
import sys
from urllib.parse import urlparse
try:
    from hdfs import InsecureClient
    HDFS_AVAILABLE = True
except ImportError:
    HDFS_AVAILABLE = False
    
# Spark ì˜ì¡´ì„± ì™„ì „ ì œê±°
SPARK_AVAILABLE = False

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œë¥¼ sys.pathì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
try:
    from config import settings
except ImportError:
    # ì„¤ì • íŒŒì¼ì´ ì—†ì„ ë•Œ ê¸°ë³¸ê°’ ì‚¬ìš©
    class Settings:
        SENTIMENT_DATA_PATH = "hdfs://namenode:8020/user/spark/social_sentiment_data"
    settings = Settings()

# --- í˜ì´ì§€ ì„¤ì • ---
st.set_page_config(
    page_title="ì‹¤ì‹œê°„ ì†Œì…œ ë¯¸ë””ì–´ ê°ì„± ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
    page_icon="ğŸ“Š",
    layout="wide",
)

# --- HDFS ì§ì ‘ ì—°ê²° ì‚¬ìš© (Spark ì—†ì´) ---
@st.cache_resource
def get_hdfs_client():
    """HDFS í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not HDFS_AVAILABLE:
        st.warning("âš ï¸ HDFS ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. WebHDFS APIë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return None
    try:
        # Docker ë„¤íŠ¸ì›Œí¬ ë‚´ë¶€ URL ëŒ€ì‹  localhost ì‚¬ìš©
        hdfs_url = "http://localhost:9870"  # WebHDFS ì ‘ê·¼
        client = InsecureClient(hdfs_url, user="spark")
        return client
    except Exception as e:
        st.error(f"HDFS í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

# --- ëŒ€ì•ˆ: WebHDFS API ì§ì ‘ í˜¸ì¶œ ---
def list_hdfs_files(path):
    """WebHDFS APIë¥¼ ì‚¬ìš©í•˜ì—¬ HDFS íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    webhdfs_urls = [
        "http://localhost:9870/webhdfs/v1",
        "http://127.0.0.1:9870/webhdfs/v1",
        "http://namenode:9870/webhdfs/v1"
    ]
    
    st.write(f"ğŸŒ HDFS ì—°ê²° ì‹œë„ ì¤‘... (ê²½ë¡œ: {path})")
    
    for i, webhdfs_url in enumerate(webhdfs_urls):
        try:
            st.write(f"ğŸ”— ì‹œë„ {i+1}: {webhdfs_url}")
            response = requests.get(f"{webhdfs_url}{path}", params={"op": "LISTSTATUS"}, timeout=5)
            st.write(f"ğŸ“¡ ì‘ë‹µ ìƒíƒœ: {response.status_code}")
            if response.status_code == 200:
                files = response.json().get("FileStatuses", {}).get("FileStatus", [])
                st.write(f"âœ… ì„±ê³µ! {len(files)}ê°œ íŒŒì¼ ë°œê²¬")
                return files
            else:
                st.write(f"âŒ ì‹¤íŒ¨: HTTP {response.status_code}")
        except Exception as e:
            st.write(f"âŒ ì—°ê²° ì˜¤ë¥˜: {str(e)[:50]}")
            continue
    
    st.error(f"WebHDFS API í˜¸ì¶œ ì‹¤íŒ¨: ëª¨ë“  URL ì—°ê²° ì‹œë„ ì‹¤íŒ¨")
    return []

def read_hdfs_file_content(file_path):
    """WebHDFS APIë¥¼ ì‚¬ìš©í•˜ì—¬ HDFS íŒŒì¼ ë‚´ìš©ì„ ì½ì–´ì˜µë‹ˆë‹¤."""
    try:
        webhdfs_url = "http://localhost:9870/webhdfs/v1"
        response = requests.get(f"{webhdfs_url}{file_path}", params={"op": "OPEN"}, timeout=30)
        if response.status_code == 200:
            return response.content
        return None
    except Exception as e:
        st.error(f"HDFS íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
        return None

def load_data_from_hdfs():
    """WebHDFS APIë¥¼ ì‚¬ìš©í•˜ì—¬ HDFSì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    # ë””ë²„ê¹…ìš© ì¶œë ¥
    st.write("ğŸ”„ load_data_from_hdfs() í•¨ìˆ˜ ì‹¤í–‰ë¨")
    try:
        hdfs_path = "/user/spark/social_sentiment_data"  # HDFS ê²½ë¡œ
        
        # HDFS ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        files = list_hdfs_files(hdfs_path)
        st.write(f"ğŸ” list_hdfs_files() ê²°ê³¼: {len(files) if files else 0}ê°œ íŒŒì¼")
        
        if not files:
            st.warning("âš ï¸ HDFSì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ì•„ì§ ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
            return pd.DataFrame()
        
        # parquet íŒŒì¼ë§Œ í•„í„°ë§
        parquet_files = [f for f in files if f.get('pathSuffix', '').endswith('.parquet') and f.get('length', 0) > 0]
        st.write(f"ğŸ¯ Parquet íŒŒì¼ í•„í„°ë§ ê²°ê³¼: {len(parquet_files)}ê°œ íŒŒì¼")
        
        if not parquet_files:
            st.warning(f"âš ï¸ {len(files)}ê°œ íŒŒì¼ì´ ìˆì§€ë§Œ ìœ íš¨í•œ Parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            st.info("ì•„ì§ ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì´ ì‹¤í–‰ë˜ê³  ë°ì´í„°ê°€ ìˆ˜ì§‘ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
            return pd.DataFrame()
        
        # ì‹¤ì œ Parquet íŒŒì¼ì—ì„œ ë°ì´í„° ì½ê¸° ì‹œë„
        st.success(f"ğŸ“ {len(parquet_files)}ê°œì˜ Parquet íŒŒì¼ì„ HDFSì—ì„œ ë°œê²¬í–ˆìŠµë‹ˆë‹¤!")
        
        # íŒŒì¼ ì •ë³´ ìš”ì•½ í‘œì‹œ
        total_size = sum(f.get('length', 0) for f in parquet_files)
        st.write(f"ğŸ“Š ì´ ë°ì´í„° í¬ê¸°: {total_size:,} bytes")
        
        # ì „ì²´ íŒŒì¼ì„ ì½ë˜, í¬ê¸°ê°€ í° íŒŒì¼ë“¤ ìš°ì„  (ìµœëŒ€ 10ê°œ)
        # íŒŒì¼ í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì•ˆì •ì ì¸ ë°ì´í„° í™•ë³´
        latest_files = sorted(parquet_files, key=lambda x: x.get('length', 0), reverse=True)[:10]
        st.info(f"ğŸ“Š ì´ {len(parquet_files)}ê°œ íŒŒì¼ ì¤‘ ìƒìœ„ {len(latest_files)}ê°œ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ")
        
        all_data = []
        successful_reads = 0
        
        for file_info in latest_files:
            file_name = file_info.get('pathSuffix', '')
            file_path = f"{hdfs_path}/{file_name}"
            
            try:
                # WebHDFS APIë¡œ íŒŒì¼ ë‚´ìš© ì½ê¸° (namenode ì‚¬ìš©)
                webhdfs_urls = [
                    "http://namenode:9870/webhdfs/v1",
                    "http://localhost:9870/webhdfs/v1", 
                    "http://127.0.0.1:9870/webhdfs/v1"
                ]
                
                response = None
                for webhdfs_url in webhdfs_urls:
                    try:
                        response = requests.get(f"{webhdfs_url}{file_path}", params={"op": "OPEN"}, timeout=30)
                        if response.status_code == 200:
                            break
                    except requests.exceptions.RequestException:
                        continue
                
                if response and response.status_code == 200:
                    # parquet íŒŒì¼ì„ ë©”ëª¨ë¦¬ì—ì„œ ì½ê¸°
                    import io
                    parquet_buffer = io.BytesIO(response.content)
                    
                    try:
                        df_chunk = pd.read_parquet(parquet_buffer)
                        all_data.append(df_chunk)
                        successful_reads += 1
                        st.write(f"âœ… ì½ê¸° ì„±ê³µ: {file_name} ({len(df_chunk)} ë ˆì½”ë“œ)")
                    except Exception as e:
                        st.write(f"âš ï¸ Parquet íŒŒì‹± ì‹¤íŒ¨: {file_name} - {str(e)[:50]}")
                else:
                    status_code = response.status_code if response else "ì—°ê²° ì‹¤íŒ¨"
                    st.write(f"âŒ íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {file_name} - HTTP {status_code}")
                    
            except Exception as e:
                st.write(f"âŒ ì—°ê²° ì˜¤ë¥˜: {file_name} - {str(e)[:50]}")
        
        if all_data and successful_reads > 0:
            # ëª¨ë“  ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
            combined_df = pd.concat(all_data, ignore_index=True)
            
            # ì‹œê°„ëŒ€ ì •ë³´ ì²˜ë¦¬ - Sparkì—ì„œ ì´ë¯¸ í•œêµ­ì‹œê°„ìœ¼ë¡œ ë³€í™˜ë˜ì–´ ì €ì¥ë¨
            if 'created_at' in combined_df.columns:
                try:
                    # created_atì´ ë¬¸ìì—´ì¸ ê²½ìš° datetimeìœ¼ë¡œ ë³€í™˜
                    if combined_df['created_at'].dtype == 'object':
                        combined_df['created_at'] = pd.to_datetime(combined_df['created_at'])
                    
                    # Sparkì—ì„œ ì´ë¯¸ Asia/Seoulë¡œ ë³€í™˜í–ˆìœ¼ë¯€ë¡œ ì‹œê°„ëŒ€ ì •ë³´ë§Œ ì¶”ê°€
                    if combined_df['created_at'].dt.tz is None:
                        combined_df['created_at'] = combined_df['created_at'].dt.tz_localize('Asia/Seoul')
                    
                    st.info(f"â° ì‹œê°„ ì²˜ë¦¬ ì™„ë£Œ. ìƒ˜í”Œ ì‹œê°„: {combined_df['created_at'].iloc[0]}")
                    
                except Exception as e:
                    st.warning(f"ì‹œê°„ëŒ€ ë³€í™˜ ì‹¤íŒ¨: {e}")
            
            # ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬
            if 'created_at' in combined_df.columns:
                combined_df = combined_df.sort_values(by="created_at", ascending=False)
            
            st.success(f"ğŸ‰ ì´ {len(combined_df)}ê°œì˜ ì‹¤ì œ ë°ì´í„°ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤!")
            return combined_df
            
        else:
            # ë°ì´í„° ì½ê¸° ì‹¤íŒ¨ ì‹œ ê²½ê³  ë©”ì‹œì§€ë§Œ í‘œì‹œ
            st.warning("âš ï¸ ì‹¤ì œ ë°ì´í„°ë¥¼ ì½ì„ ìˆ˜ ì—†ì–´ ë¹ˆ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            st.info("ë°ì´í„° íŒŒì´í”„ë¼ì¸ì´ ì‹¤í–‰ ì¤‘ì´ê±°ë‚˜ parquet íŒŒì¼ í˜•ì‹ì— ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
    except Exception as e:
        st.error(f"HDFS ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame()


# --- ëŒ€ì‹œë³´ë“œ UI ---
st.title("ğŸ“Š ì‹¤ì‹œê°„ ì†Œì…œ ë¯¸ë””ì–´ ê°ì„± ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

# ìºì‹œ ì™„ì „ ì´ˆê¸°í™” (ë§¤ë²ˆ ìƒˆë¡œìš´ ë°ì´í„° ë¡œë“œ)
st.cache_data.clear()
st.cache_resource.clear()

# ê°•ì œ ë¦¬í”„ë ˆì‹œë¥¼ ìœ„í•œ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ê°€
current_time = int(time.time())
st.write(f"ğŸ• ë¡œë“œ ì‹œê°„: {current_time}")

df = load_data_from_hdfs()

if df.empty:
    st.warning("ì•„ì§ ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. íŒŒì´í”„ë¼ì¸ì´ ì‹¤í–‰ë˜ê³  ë°ì´í„°ê°€ ìˆ˜ì§‘ë  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.")
    
    # ë°ì´í„° ìˆ˜ì§‘ ìƒíƒœ ì²´í¬
    with st.expander("íŒŒì´í”„ë¼ì¸ ìƒíƒœ ì²´í¬"):
        try:
            # WebHDFS APIë¥¼ ì‚¬ìš©í•œ ìƒíƒœ ì²´í¬ (ì—¬ëŸ¬ URL ì‹œë„)
            webhdfs_urls = [
                "http://localhost:9870/webhdfs/v1",
                "http://127.0.0.1:9870/webhdfs/v1",
                "http://namenode:9870/webhdfs/v1"  # Docker ë„¤íŠ¸ì›Œí¬ ë‚´ë¶€ì—ì„œ ì‹¤í–‰ ì‹œ
            ]
            
            # HDFS ì—°ê²° ì²´í¬ (ì—¬ëŸ¬ URL ì‹œë„)
            connected = False
            working_url = None
            
            for webhdfs_url in webhdfs_urls:
                try:
                    response = requests.get(f"{webhdfs_url}/", params={"op": "LISTSTATUS"}, timeout=3)
                    if response.status_code == 200:
                        st.success(f"âœ… HDFS WebHDFS ì—°ê²° ì„±ê³µ: {webhdfs_url}")
                        working_url = webhdfs_url
                        connected = True
                        break
                except requests.exceptions.RequestException:
                    continue
            
            if connected:
                # ë°ì´í„° ë””ë ‰í† ë¦¬ ì²´í¬
                data_files = list_hdfs_files("/user/spark/social_sentiment_data")
                if data_files:
                    parquet_count = len([f for f in data_files if f.get('pathSuffix', '').endswith('.parquet')])
                    st.info(f"ğŸ“ ë°ì´í„° ë””ë ‰í† ë¦¬ ì¡´ì¬: {len(data_files)}ê°œ íŒŒì¼ ({parquet_count}ê°œ Parquet)")
                else:
                    st.warning("âš ï¸ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì•„ì§ ìƒì„±ë˜ì§€ ì•ŠìŒ")
            else:
                st.error("âŒ ëª¨ë“  HDFS URLì— ì—°ê²° ì‹¤íŒ¨")
                st.info("ğŸ”§ ì‹œë„í•œ URL: " + ", ".join(webhdfs_urls))
                
            # Docker ì„œë¹„ìŠ¤ ìƒíƒœ ì²´í¬ ì•ˆë‚´
            st.info("ğŸ‹ Docker ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸: `docker ps` ëª…ë ¹ìœ¼ë¡œ í™•ì¸í•´ì£¼ì„¸ìš”.")
            st.info("ğŸ”Œ í•„ìš”í•œ ì„œë¹„ìŠ¤: namenode, datanode, spark-streaming-analyzer")
                
        except Exception as e:
            st.error(f"âŒ ìƒíƒœ ì²´í¬ ì‹¤íŒ¨: {str(e)[:100]}...")
else:
    total_tweets = len(df)
    
    # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
    # ì‹œê°„ í˜•ì‹ì„ ì½ê¸° ì‰½ê²Œ ë³€í™˜ (ì—°ë„ í¬í•¨)
    if 'created_at' in df.columns:
        df['formatted_time'] = df['created_at'].dt.strftime('%Y-%m-%d %H:%M')
    
    # ì‹¤ì œ sentiment_label ê°’ë“¤ í™•ì¸
    if 'sentiment_label' in df.columns:
        unique_sentiments = df['sentiment_label'].unique()
        st.info(f"ğŸ” ì‹¤ì œ ê°ì„± ë¼ë²¨ ê°’ë“¤: {list(unique_sentiments)}")
        sentiment_counts_raw = df['sentiment_label'].value_counts()
        st.write("**ê° ë¼ë²¨ ê°œìˆ˜:**")
        for label, count in sentiment_counts_raw.items():
            st.write(f"â€¢ {label}: {count}ê°œ")
    
    # ìœ íš¨í•œ ê°ì„± ë¼ë²¨ë§Œ í•„í„°ë§ (ì‹¤ì œ ê°’ì— ë§ì¶° ì¡°ì •)
    valid_sentiments = ['POSITIVE', 'NEGATIVE', 'NEUTRAL', 'positive', 'negative', 'neutral']
    df_valid = df[df['sentiment_label'].isin(valid_sentiments)].copy()
    
    # ë§Œì•½ ì—¬ì „íˆ ë¹„ì–´ìˆë‹¤ë©´ ëª¨ë“  ë°ì´í„° í‘œì‹œ (ë””ë²„ê¹…ìš©)
    if df_valid.empty and not df.empty:
        st.warning("âš ï¸ ìœ íš¨í•œ ê°ì„± ë¼ë²¨ì´ ì—†ì–´ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")
        df_valid = df.copy()
    
    # ì–¸ì–´ë³„ë¡œ ë°ì´í„° ë¶„ë¦¬
    if 'detected_language' in df.columns:
        df_korean = df_valid[df_valid['detected_language'] == 'ko'].copy()
        df_english = df_valid[df_valid['detected_language'] == 'en'].copy()
        df_other = df_valid[~df_valid['detected_language'].isin(['ko', 'en'])].copy()
    else:
        df_korean = df_valid.copy()
        df_english = pd.DataFrame()
        df_other = pd.DataFrame()
    
    # --- ìƒë‹¨ KPI ---
    col_kpi1, col_kpi2, col_kpi3 = st.columns(3)
    with col_kpi1:
        st.metric(label="ì´ ìˆ˜ì§‘ í•­ëª©", value=f"{total_tweets:,} ê°œ")
    with col_kpi2:
        st.metric(label="ìœ íš¨ ë¶„ì„ ë°ì´í„°", value=f"{len(df_valid):,} ê°œ")
    with col_kpi3:
        error_count = total_tweets - len(df_valid)
        st.metric(label="ë¶„ì„ ì˜¤ë¥˜", value=f"{error_count:,} ê°œ")
    
    # --- ê°ì„± ë¶„ì„ ê²°ê³¼ ---
    if not df_valid.empty:
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("ì „ì²´ ê°ì„± ë¶„í¬")
            sentiment_counts = df_valid['sentiment_label'].value_counts()
            st.bar_chart(sentiment_counts)

        with col2:
            st.subheader("ì‹œê°„ëŒ€ë³„ í•­ëª©ëŸ‰")
            if 'created_at' in df_valid.columns:
                df_valid['hour'] = df_valid['created_at'].dt.hour
                hourly_counts = df_valid.groupby('hour').size()
                st.line_chart(hourly_counts)

    # --- ì–¸ì–´ë³„ ë°ì´í„° í‘œì‹œ ---
    tab1, tab2, tab3 = st.tabs(["ğŸ‡°ğŸ‡· í•œêµ­ì–´", "ğŸ‡ºğŸ‡¸ ì˜ì–´", "ğŸŒ ê¸°íƒ€"])
    
    with tab1:
        if not df_korean.empty:
            st.subheader(f"í•œêµ­ì–´ ë°ì´í„° ({len(df_korean)}ê°œ)")
            
            # í•œêµ­ì–´ ê°ì„± ë¶„í¬
            if len(df_korean) > 0:
                korean_sentiment = df_korean['sentiment_label'].value_counts()
                col_k1, col_k2 = st.columns(2)
                with col_k1:
                    st.write("**ê°ì„± ë¶„í¬**")
                    st.bar_chart(korean_sentiment)
                with col_k2:
                    st.write("**ê°ì„± ë¹„ìœ¨**")
                    for sentiment, count in korean_sentiment.items():
                        percentage = (count / len(df_korean)) * 100
                        st.write(f"â€¢ {sentiment}: {count}ê°œ ({percentage:.1f}%)")
            
            # ìµœì‹  í•œêµ­ì–´ ë°ì´í„°
            st.write("**ìµœì‹  í•œêµ­ì–´ í•­ëª©**")
            display_columns = ['formatted_time', 'text', 'sentiment_label']
            available_columns = [col for col in display_columns if col in df_korean.columns]
            if available_columns:
                st.dataframe(
                    df_korean[available_columns].head(10),
                    column_config={
                        'formatted_time': 'ì‹œê°„',
                        'text': 'ë‚´ìš©', 
                        'sentiment_label': 'ê°ì„±'
                    }
                )
        else:
            st.info("í•œêµ­ì–´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    with tab2:
        if not df_english.empty:
            st.subheader(f"ì˜ì–´ ë°ì´í„° ({len(df_english)}ê°œ)")
            
            # ì˜ì–´ ê°ì„± ë¶„í¬
            if len(df_english) > 0:
                english_sentiment = df_english['sentiment_label'].value_counts()
                col_e1, col_e2 = st.columns(2)
                with col_e1:
                    st.write("**Sentiment Distribution**")
                    st.bar_chart(english_sentiment)
                with col_e2:
                    st.write("**Sentiment Ratio**")
                    for sentiment, count in english_sentiment.items():
                        percentage = (count / len(df_english)) * 100
                        st.write(f"â€¢ {sentiment}: {count} items ({percentage:.1f}%)")
            
            # ìµœì‹  ì˜ì–´ ë°ì´í„°  
            st.write("**Latest English Items**")
            display_columns = ['formatted_time', 'text', 'sentiment_label']
            available_columns = [col for col in display_columns if col in df_english.columns]
            if available_columns:
                st.dataframe(
                    df_english[available_columns].head(10),
                    column_config={
                        'formatted_time': 'Time',
                        'text': 'Content', 
                        'sentiment_label': 'Sentiment'
                    }
                )
        else:
            st.info("No English data available.")
            
    with tab3:
        if not df_other.empty:
            st.subheader(f"ê¸°íƒ€ ì–¸ì–´ ë°ì´í„° ({len(df_other)}ê°œ)")
            display_columns = ['formatted_time', 'text', 'sentiment_label', 'detected_language']
            available_columns = [col for col in display_columns if col in df_other.columns]
            if available_columns:
                st.dataframe(df_other[available_columns].head(10))
        else:
            st.info("ê¸°íƒ€ ì–¸ì–´ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ìë™ ìƒˆë¡œê³ ì¹¨ ì„¹ì…˜ì„ ì¡°ê±´ë¶€ë¡œ ì²˜ë¦¬
if not df.empty:
    # ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ ì‹œê°„ í‘œì‹œ
    last_update = df['created_at'].max() if 'created_at' in df.columns else "ì•Œ ìˆ˜ ì—†ìŒ"
    st.info(f"ğŸ“Š ë§ˆì§€ë§‰ ë°ì´í„° ì—…ë°ì´íŠ¸: {last_update}")
    st.info("ëŒ€ì‹œë³´ë“œëŠ” 30ì´ˆë§ˆë‹¤ ìë™ìœ¼ë¡œ ìƒˆë¡œê³ ì¹¨ë©ë‹ˆë‹¤.")

# ìë™ ìƒˆë¡œê³ ì¹¨ (ì¡°ê±´ë¶€) - ë” ë¹ ë¥¸ ìƒˆë¡œê³ ì¹¨
auto_refresh = st.checkbox("ìë™ ìƒˆë¡œê³ ì¹¨ í™œì„±í™”", value=True)  # ê¸°ë³¸ê°’ trueë¡œ ë³€ê²½
if auto_refresh:
    placeholder = st.empty()
    time.sleep(30)  # 30ì´ˆë¡œ ë‹¨ì¶•
    st.rerun()