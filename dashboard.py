import streamlit as st
import pandas as pd
import json
import logging
import os
import io
from hdfs import InsecureClient
from datetime import timedelta

# --- í™˜ê²½ ë³€ìˆ˜ ë° ê¸°ë³¸ ì„¤ì • ---
# Docker ì»¨í…Œì´ë„ˆ í™˜ê²½ì—ì„œ HDFSì™€ ì—°ê²°í•˜ê¸° ìœ„í•œ ì„¤ì •ë“¤
HDFS_NAMENODE_HOST = os.getenv('HDFS_NAMENODE_HOST', 'namenode')
HDFS_WEB_PORT = int(os.getenv('HDFS_WEB_PORT', 9870))
DASHBOARD_REFRESH_SECONDS = int(os.getenv('DASHBOARD_REFRESH_SECONDS', 30))

# HDFSì— ì €ì¥ëœ ë°ì´í„° ê²½ë¡œë“¤ - Sparkê°€ parquet íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ìœ„ì¹˜
HDFS_DATA_PATH = "/user/spark/social_sentiment_data"  # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°
REPORT_PATH = "/user/spark/daily_sentiment_reports"   # ì¼ë³„ ì§‘ê³„ ë¦¬í¬íŠ¸

# HDFS í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” - WebHDFS API ì‚¬ìš©
client = InsecureClient(f'http://{HDFS_NAMENODE_HOST}:{HDFS_WEB_PORT}', user='root')

# ë¡œê¹… ì„¤ì • - ëŒ€ì‹œë³´ë“œ ì‹¤í–‰ ìƒíƒœ ëª¨ë‹ˆí„°ë§ìš©
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- ë°ì´í„° ë¡œë”© í•¨ìˆ˜ ---
def load_hdfs_data(hdfs_path):
    """
    HDFSì—ì„œ ëª¨ë“  Parquet íŒŒì¼ì„ ì½ì–´ Pandas DataFrameìœ¼ë¡œ ë°˜í™˜
    
    Spark Streamingì´ ìƒì„±í•œ íŒŒí‹°ì…˜ë³„ parquet íŒŒì¼ë“¤ì„ 
    ëª¨ë‘ ì½ì–´ì„œ í•˜ë‚˜ì˜ DataFrameìœ¼ë¡œ í•©ì¹˜ëŠ” í•¨ìˆ˜
    """
    try:
        # HDFS ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if not client.status(hdfs_path, strict=False):
            logger.warning(f"HDFS ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {hdfs_path}")
            return pd.DataFrame()

        # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        file_list = client.list(hdfs_path)
        # .parquet í™•ì¥ìë¥¼ ê°€ì§„ íŒŒì¼ë“¤ë§Œ í•„í„°ë§
        parquet_files = [f for f in file_list if f.endswith('.parquet')]
        
        if not parquet_files:
            return pd.DataFrame()

        # ê° parquet íŒŒì¼ì„ ì½ì–´ì„œ DataFrame ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        all_dfs = []
        for file_name in parquet_files:
            full_path = os.path.join(hdfs_path, file_name)
            # HDFSì—ì„œ íŒŒì¼ ì½ê¸°
            with client.read(full_path) as reader:
                # ë©”ëª¨ë¦¬ ë²„í¼ì— ì €ì¥ í›„ pandasë¡œ ì½ê¸°
                buffer = io.BytesIO(reader.read())
                df = pd.read_parquet(buffer)
                all_dfs.append(df)
        
        if not all_dfs:
            return pd.DataFrame()
            
        # ëª¨ë“  DataFrameì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
        return pd.concat(all_dfs, ignore_index=True)

    except Exception as e:
        logger.error(f"HDFS ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        st.error(f"HDFSì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return pd.DataFrame()

# --- Streamlit UI êµ¬ì„± ---
st.set_page_config(layout="wide")
st.title("ì†Œì…œ ë¯¸ë””ì–´ ê°ì„± ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

# --- ë°ì´í„° ë¡œë”© ---
# ë°°ì¹˜ ì²˜ë¦¬ëœ ì¼ì¼ ë¦¬í¬íŠ¸ì™€ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ë¥¼ ê°ê° ë¡œë“œ
report_df = load_hdfs_data(REPORT_PATH)        # Airflow DAGë¡œ ìƒì„±ëœ ì¼ë³„ ì§‘ê³„ ë°ì´í„°
latest_data_df = load_hdfs_data(HDFS_DATA_PATH)  # Spark Streamingìœ¼ë¡œ ì‹¤ì‹œê°„ ìˆ˜ì§‘ëœ ë°ì´í„°

# --- ì¼ì¼ ë¦¬í¬íŠ¸ ì„¹ì…˜ ---
st.header("ğŸ“Š ì¼ì¼ ê°ì„± ë¶„ì„ ë¦¬í¬íŠ¸")
if not report_df.empty:
    # UTC ì‹œê°„ì„ datetimeìœ¼ë¡œ ë³€í™˜í•˜ê³  ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬
    report_df['report_date'] = pd.to_datetime(report_df['report_date'])
    report_df = report_df.sort_values(by='report_date', ascending=False)
    
    if not report_df.empty:
        # ê°€ì¥ ìµœì‹  ë¦¬í¬íŠ¸ ë°ì´í„° ì„ íƒ
        latest_report = report_df.iloc[0]
        
        # UTC â†’ KST ì‹œê°„ëŒ€ ë³€í™˜ (í•œêµ­ ì‹œê°„ í‘œì‹œìš©)
        kst_time = latest_report['report_date'] + timedelta(hours=9)

        # ë©”íŠ¸ë¦­ ì¹´ë“œë¡œ ì£¼ìš” ì§€í‘œ í‘œì‹œ
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ë¦¬í¬íŠ¸ ìƒì„± ì‹œê°„ (KST)", kst_time.strftime('%Y-%m-%d %H:%M:%S'))
        with col2:
            st.metric("í‰ê·  ê°ì„± ì ìˆ˜", f"{latest_report['average_sentiment_score']:.4f}")
        
        # ê°ì„± ì ìˆ˜ ê³„ì‚° ë°©ì‹ ì„¤ëª…
        with st.expander("â„¹ï¸ í‰ê·  ê°ì„± ì ìˆ˜ ì‚°ì • ê¸°ì¤€ ë³´ê¸°"):
            st.markdown("""
            - **Positive:** +1ì 
            - **Negative:** -1ì   
            - **Neutral (ë˜ëŠ” ê¸°íƒ€):** 0ì 
            
            ìœ„ ê¸°ì¤€ìœ¼ë¡œ ì§€ë‚œ 24ì‹œê°„ ë™ì•ˆ ìˆ˜ì§‘ëœ ëª¨ë“  ë°ì´í„°ì˜ ê°ì„± ì ìˆ˜ í‰ê· ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
            """)

        # í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
        st.subheader("ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œ Top 10")
        try:
            # JSON í˜•íƒœë¡œ ì €ì¥ëœ í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ íŒŒì‹±
            top_keywords = json.loads(latest_report['top_keywords'])
            st.table(pd.DataFrame(top_keywords))
        except json.JSONDecodeError:
            st.error("í‚¤ì›Œë“œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    else:
        st.warning("ì²˜ë¦¬í•  ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.warning("ì•„ì§ ìƒì„±ëœ ì¼ì¼ ë¦¬í¬íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

# --- ì‹¤ì‹œê°„ ë°ì´í„° ì„¹ì…˜ ---
st.header("ğŸ•’ ì‹¤ì‹œê°„ ë°ì´í„° ë¶„ì„")
if not latest_data_df.empty:
    latest_data_df['created_at'] = pd.to_datetime(latest_data_df['created_at'])
    latest_data_df = latest_data_df.sort_values(by="created_at", ascending=False)

    total_tweets = len(latest_data_df)
    
    df_korean = latest_data_df[latest_data_df['detected_language'] == 'ko'].copy()
    df_english = latest_data_df[latest_data_df['detected_language'] == 'en'].copy()
    df_other = latest_data_df[~latest_data_df['detected_language'].isin(['ko', 'en'])].copy()

    col_kpi1, col_kpi2, col_kpi3, col_kpi4 = st.columns(4)
    with col_kpi1:
        st.metric(label="ì „ì²´ ìˆ˜ì§‘ ë°ì´í„°", value=f"{total_tweets:,} ê±´")
    with col_kpi2:
        st.metric(label="í•œêµ­ì–´ ë°ì´í„°", value=f"{len(df_korean):,} ê±´")
    with col_kpi3:
        st.metric(label="ì˜ì–´ ë°ì´í„°", value=f"{len(df_english):,} ê±´")
    with col_kpi4:
        st.metric(label="ê¸°íƒ€ ì–¸ì–´ ë°ì´í„°", value=f"{len(df_other):,} ê±´")

    tab1, tab2, tab3 = st.tabs([f"ğŸ‡°ğŸ‡· í•œêµ­ì–´ ({len(df_korean)}ê±´)", f"ğŸ‡ºğŸ‡¸ ì˜ì–´ ({len(df_english)}ê±´)", f"ğŸŒ ê¸°íƒ€ ({len(df_other)}ê±´)"])

    def display_language_dashboard(tab, df, lang_name):
        with tab:
            if not df.empty:
                col1, col2 = st.columns([0.7, 0.3])
                with col1:
                    st.subheader(f"{lang_name} ìµœì‹  ë°ì´í„°")
                    display_cols = ['created_at', 'text', 'sentiment_label']
                    if lang_name == "ê¸°íƒ€ ì–¸ì–´":
                        display_cols.append('detected_language')
                    st.dataframe(df[display_cols].head(10))
                with col2:
                    st.subheader("ê°ì„± ë¶„í¬")
                    sentiment_counts = df['sentiment_label'].value_counts()
                    st.bar_chart(sentiment_counts)
            else:
                st.info(f"{lang_name} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    display_language_dashboard(tab1, df_korean, "í•œêµ­ì–´")
    display_language_dashboard(tab2, df_english, "ì˜ì–´")
    display_language_dashboard(tab3, df_other, "ê¸°íƒ€ ì–¸ì–´")

else:
    st.info("ìˆ˜ì§‘ëœ ì‹¤ì‹œê°„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ìë™ ìƒˆë¡œê³ ì¹¨
st.button("ìƒˆë¡œê³ ì¹¨")