services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    networks:
      - social_media_network
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    networks:
      - social_media_network
    depends_on:
      zookeeper:
        condition: service_started
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      # 로그 레벨을 낮춰 불필요한 로그 출력 방지
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Kafka 토픽을 명시적으로 생성하는 서비스
  kafka-topic-creator:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka-topic-creator
    networks:
      - social_media_network
    depends_on:
      kafka:
        condition: service_healthy
    environment:
      # Log4j 권한 오류 해결을 위한 환경 변수
      KAFKA_OPTS: "-Dlog4j.configuration=file:/etc/kafka/log4j.properties"
    command: >
      bash -c "
        echo 'Waiting for Kafka to be ready...' &&
        sleep 10 &&
        cub kafka-ready -b kafka:9092 1 60 &&
        echo 'Kafka is ready!' &&
        kafka-topics --create --topic reddit-stream --partitions 1 --replication-factor 1 --if-not-exists --bootstrap-server kafka:9092 &&
        echo 'Topic created successfully!' &&
        exit 0
      "
    restart: "no"

  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    restart: always
    networks:
      - social_media_network
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    ports:
      - "9870:9870"
      - "9000:9000"
      - "8020:8020"
    environment:
      - CLUSTER_NAME=test
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020
      - HDFS_CONF_dfs_permissions_enabled=false # 환경변수로 권한 설정
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-safemode", "get"]
      interval: 5s
      timeout: 5s
      retries: 10

  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    restart: always
    networks:
      - social_media_network
    depends_on:
      - namenode
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    environment:
      - CORE_CONF_fs_defaultFS=hdfs://namenode:8020 # Namenode 주소 설정
    healthcheck:
      test: ["CMD", "hdfs", "dfsadmin", "-Dfs.defaultFS=hdfs://namenode:8020", "-report"]
      interval: 5s
      timeout: 5s
      retries: 10

  spark-master:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-master
    networks:
      - social_media_network
    volumes:
      - ./scripts:/opt/bitnami/spark/scripts
    ports:
      - "8080:8080"
      - "7077:7077"
    environment:
      - SPARK_MODE=master

  spark-worker:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker
    networks:
      - social_media_network
    depends_on:
      - spark-master
    volumes:
      - ./scripts:/opt/bitnami/spark/scripts
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077

  spark-worker-2:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-worker-2
    networks:
      - social_media_network
    depends_on:
      - spark-master
    volumes:
      - ./scripts:/opt/bitnami/spark/scripts
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077

  reddit-producer:
    build: .
    container_name: reddit-producer
    networks:
      - social_media_network
    depends_on:
      kafka-topic-creator:
        condition: service_completed_successfully
    environment:
      # 환경 설정
      - ENVIRONMENT=development
      - LOG_LEVEL=INFO
      
      # Reddit API 설정 (실제 사용 시 .env 파일이나 환경변수로 설정)
      - REDDIT_CLIENT_ID=${REDDIT_CLIENT_ID:-your_client_id_here}
      - REDDIT_CLIENT_SECRET=${REDDIT_CLIENT_SECRET:-your_client_secret_here}
      - REDDIT_USERNAME=${REDDIT_USERNAME:-your_username_here}
      - REDDIT_PASSWORD=${REDDIT_PASSWORD:-your_password_here}
      - REDDIT_USER_AGENT=social_media_analysis:v2.0 (by /u/dev_user)
      
      # Kafka 설정
      - KAFKA_BROKER=kafka:9092
      - KAFKA_TOPIC=reddit-stream
      - SUBREDDIT_NAME=all
      
    volumes:
      - ./scripts:/opt/airflow/scripts
      - ./config:/opt/airflow/config
    command: python /opt/airflow/scripts/reddit_producer.py
    restart: always

  spark-streaming-analyzer:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: spark-streaming-analyzer
    networks:
      - social_media_network
    depends_on:
      kafka-topic-creator:
        condition: service_completed_successfully
      spark-master:
        condition: service_started
      namenode:
        condition: service_healthy
    volumes:
      - ./scripts:/opt/bitnami/spark/scripts
      - ./config:/opt/bitnami/spark/config
      - ./requirements-spark.txt:/tmp/requirements-spark.txt
    user: root
    environment:
      # 환경 설정
      - ENVIRONMENT=development
      - LOG_LEVEL=INFO
      
      # API 키 (실제 사용 시 .env 파일이나 환경변수로 설정)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-dummy_key_for_development}
      - HUGGINGFACE_API_KEY=${HUGGINGFACE_API_KEY:-dummy_key_for_development}
      
      # 캐시 및 모델 경로
      - HF_HOME=/tmp/huggingface_cache
      - TRANSFORMERS_CACHE=/tmp/transformers_cache
      
      # 성능 최적화된 Spark 설정
      - SPARK_EXECUTOR_MEMORY=4g
      - SPARK_DRIVER_MEMORY=2g
      - SPARK_EXECUTOR_CORES=4
      - SPARK_BATCH_INTERVAL=30
      - MAX_FILES_PER_LOAD=15
      - PARQUET_COMPRESSION=snappy
      
      # API 제한 설정
      - MAX_API_REQUESTS_PER_MINUTE=50
      - API_TIMEOUT_SECONDS=10
      
      # 대시보드 설정
      - DASHBOARD_REFRESH_SECONDS=30

      - PYTHONPATH=/opt/bitnami/spark:/opt/bitnami/spark/scripts:/opt/bitnami/spark/config
      
    command: >
      /opt/bitnami/spark/bin/spark-submit
      --master spark://spark-master:7077
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1
      --conf spark.driver.extraJavaOptions="-Duser.timezone=UTC"
      --conf spark.executor.extraJavaOptions="-Duser.timezone=UTC"
      --total-executor-cores 12
      --py-files /opt/bitnami/spark/scripts/advanced_sentiment_analyzer.py
      /opt/bitnami/spark/scripts/sentiment_analyzer_v2.py
    restart: always
    deploy:
      resources:
        limits:
          memory: 5G  # 메모리 증가
          cpus: '4'   # CPU 코어 제한
        reservations:
          memory: 3G  # 예약 메모리 증가
          cpus: '2'   # 예약 CPU

  airflow-init:
    build: .
    container_name: airflow-init
    env_file:
      - .env
    volumes:
      - ./:/opt/airflow
      - airflow_data:/opt/airflow/logs
    entrypoint: /bin/bash
    command: -c "chown -R 50000:0 /opt/airflow/logs && airflow db migrate && airflow users create --username admin --password admin --role Admin --email admin@example.com --firstname default --lastname user || true"
    restart: "no"

  airflow-webserver:
    build: .
    container_name: airflow-webserver
    networks:
      - social_media_network
    restart: always
    depends_on:
      - airflow-init
    env_file:
      - .env
    volumes:
      - ./:/opt/airflow
      - airflow_data:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8082:8080"
    command: airflow webserver

  airflow-scheduler:
    build: .
    container_name: airflow-scheduler
    networks:
      - social_media_network
    restart: always
    depends_on:
      - airflow-init
    env_file:
      - .env
    volumes:
      - ./:/opt/airflow
      - airflow_data:/opt/airflow/logs
      - /var/run/docker.sock:/var/run/docker.sock
    command: airflow scheduler

  dashboard:
    build:
      context: .
      dockerfile: Dockerfile.dashboard
    container_name: dashboard
    networks:
      - social_media_network
    restart: always
    volumes:
      - ./:/opt/airflow
    ports:
      - "8501:8501"
    environment:
      # 환경 설정 (대시보드용)
      - ENVIRONMENT=development
      - LOG_LEVEL=INFO
      
      # 대시보드는 Reddit API가 필요하지 않으므로 더미 값 사용
      - REDDIT_CLIENT_ID=dashboard_dummy_id
      - REDDIT_CLIENT_SECRET=dashboard_dummy_secret
      - REDDIT_USERNAME=dashboard_dummy_user
      - REDDIT_PASSWORD=dashboard_dummy_pass
      
      # HDFS 설정 (대시보드에서 필요)
      - HDFS_NAMENODE_HOST=namenode
      - HDFS_NAMENODE_PORT=8020
      - HDFS_WEB_PORT=9870
      
      # 대시보드 설정
      - DASHBOARD_REFRESH_SECONDS=30
      - MAX_FILES_PER_LOAD=10
      
    entrypoint: streamlit
    command: run /opt/airflow/dashboard.py --server.port 8501 --server.address 0.0.0.0

volumes:
  airflow_data:
  hadoop_namenode:
  hadoop_datanode:

networks:
  social_media_network:
